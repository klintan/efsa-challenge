{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import re, string\n",
    "import random\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"AHAW VBD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(\"feature_descriptors.csv\",dtype=str, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"dataset/Extracted data element training set.csv\", dtype=str, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_data = pd.read_csv(topic + \".csv\",dtype=str, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_features = features.loc[features['topic']==topic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description_mapping = {row[\"feature\"]:row['description'] for _, row in topic_features.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"feature_description_mapping.json\", \"w\") as fdm:\n",
    "    fdm.write(json.dumps(feature_description_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_columns = [\"Refid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = training_data.loc[training_data['topic']==topic][list(topic_features['feature']) + base_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.merge(df_topic_data, on=\"Refid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"small_squad_sample.json\") as f:\n",
    "    squad_template = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#squad_template['data'][0]['title'] = topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodetoascii(text):\n",
    "\n",
    "    TEXT = (text.\n",
    "            replace('\\\\n', \" \").\n",
    "    \t\treplace('\\\\xe2\\\\x80\\\\x99', \"'\").\n",
    "            replace('\\\\xc3\\\\xa9', 'e').\n",
    "            replace('\\\\xe2\\\\x80\\\\x90', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x91', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x92', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x93', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x94', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x94', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x98', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\x9b', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\x9c', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\x9c', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\x9d', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\x9e', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\x9f', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\xa6', '...').#\n",
    "            replace('\\\\xe2\\\\x80\\\\xb2', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb3', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb4', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb5', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb6', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb7', \"'\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xba', \"+\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xbb', \"-\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xbc', \"=\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xbd', \"(\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xbe', \")\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xa0', \"\").\n",
    "            replace('\\\\x0c', \"\").\n",
    "            replace('\\\\xef\\\\xac\\\\x81', \"fi\").\n",
    "            replace('\\\\ufb01', \"fi\").\n",
    "            # copyright symbol\n",
    "            replace('\\\\xc2\\\\xa9', \"\").\n",
    "            replace('\\\\xa9', \"\").\n",
    "            replace('\\\\xe2\\\\x89\\\\xa5', \"of\").\n",
    "            replace('\\\\xef\\\\xac\\\\x82', \"fl\").\n",
    "            # degree symbol\n",
    "            replace('\\\\xc2\\\\xb0', \" \").\n",
    "            replace('\\\\xb0', \" \").\n",
    "            replace('\\\\xe8', \"\").\n",
    "            replace('\\\\', \"\")\n",
    "\n",
    "    \n",
    "                 )\n",
    "    return TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentence(sentence):\n",
    "    #sentence.encode('ascii').decode('utf-8', errors='ignore')\n",
    "    sentence = re.sub(r'\\\\n', \" \", sentence).strip()\n",
    "    sentence = unicodetoascii(sentence)\n",
    "    sentence = re.sub(r'\\\\\\\\x\\w+',r'', sentence)\n",
    "    #sentence = re.sub(r'[^\\x00-\\x7f]',r'', sentence)\n",
    "    #sentence = re.sub(r'\\\\\\\\x\\w{1}\\d{1}', ' ', sentence)\n",
    "    #sentence = re.sub(r'\\\\x\\w{1}\\d{1}', ' ', sentence)\n",
    "    #sentence = re.sub(r'\\\\\\\\x\\w+', ' ', sentence)\n",
    "    #sentence = re.sub(r'\\\\\\\\x\\d{2}', ' ', sentence)\n",
    "    #sentence = re.sub(r'\\\\\\\\x\\d{1}', ' ', sentence)\n",
    "    #sentence = re.sub(r'\\s+', ' ', sentence ).strip()\n",
    "    \n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_qas_obj(q, a, index):\n",
    "    q_id = uuid.uuid4()\n",
    "    qas['question'] = q\n",
    "    qas['id'] = \"id\"\n",
    "    qas['answers'] = [{\n",
    "        \"answer_start\": index ,\n",
    "        \"text\": a\n",
    "        },{\n",
    "        \"answer_start\": index ,\n",
    "        \"text\": a\n",
    "        },{\n",
    "        \"answer_start\": index ,\n",
    "        \"text\": a\n",
    "        }]\n",
    "    return qas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_template = {\"data\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_context(sentence, index, all_sentences):\n",
    "    if len(all_sentences) > 2:\n",
    "        if index == 0:\n",
    "            context = [sentence] + [sentences[index+1]] + [all_sentences[index+2]]\n",
    "        elif index == len(all_sentences)-1:\n",
    "            context = [all_sentences[index-2]] + [all_sentences[index-1]] + [sentence]\n",
    "        else:\n",
    "            context = [all_sentences[index-1]] + [sentence] + [all_sentences[index+1]]\n",
    "        return normalize_sentence(\" \".join(context))\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paragraph_obj(context, qas):\n",
    "    return {\"context\":context,\"qas\":qas}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with code below is that it checks for the offset/index in the sentence, but when we add a new sentence, it is all out of whack\n",
    "and the offset should be difference, becuase it should always be from the beginning.\n",
    "Need to check index after we put together the offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx, sample in df_data.iterrows():\n",
    "    squad_template['data'].append({'title': sample['Bibliography'], \"paragraphs\":[]})\n",
    "    sentences = sent_tokenize(normalize_sentence(sample['raw_text']))\n",
    "    for sent_id, sentence in enumerate(sentences):\n",
    "        qas = []\n",
    "        context = create_sentence_context(sentence, sent_id, sentences)\n",
    "        for idy, attrib in enumerate(feature_description_mapping):\n",
    "            #find exact match\n",
    "            if re.search(r'\\b'+sample[attrib].lower()+r'\\b', sentence.lower()) and len(sample[attrib])>0:\n",
    "                # get index of exact match\n",
    "                #start_index = re.search(r'\\b'+sample[attrib].lower()+r'\\b', sentence.lower()).start()\n",
    "                start_index = re.search(r'\\b'+sample[attrib].lower()+r'\\b', context.lower()).start()\n",
    "                #start_index = sentence.lower().index(sample[attrib].lower())\n",
    "                q = feature_description_mapping[attrib]\n",
    "                a = sample[attrib]\n",
    "                qas.append(add_qas_obj(q, a, start_index))\n",
    "        if len(qas)>=1:\n",
    "            squad_template['data'][idx]['paragraphs'].append(create_paragraph_obj(context, qas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(squad_template['data'])\n",
    "\n",
    "train_data = squad_template['data'][:int((len(squad_template['data'])+1)*.80)] #Remaining 80% to training set\n",
    "test_data = squad_template['data'][int(len(squad_template['data'])*.80+1):] #Splits 20% data to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_wrapper = {\"data\":train_data, \"version\":\"1.1\"}\n",
    "test_data_wrapper = {\"data\":test_data, \"version\":\"1.1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dev-v1.1.json\", \"w\") as d:\n",
    "    d.write(json.dumps(test_data_wrapper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train-v1.1.json\", \"w\") as d:\n",
    "    d.write(json.dumps(train_data_wrapper))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
