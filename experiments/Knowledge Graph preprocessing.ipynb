{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "import pandas as pd\n",
    "import re\n",
    "import corenlp\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"AHAW VBD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_data = pd.read_csv(topic + \".csv\",dtype=str, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodetoascii(text):\n",
    "\n",
    "    TEXT = (text.\n",
    "            replace('\\\\n', \" \").\n",
    "    \t\treplace('\\\\xe2\\\\x80\\\\x99', \"'\").\n",
    "            replace('\\\\xc3\\\\xa9', 'e').\n",
    "            replace('\\\\xe2\\\\x80\\\\x90', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x91', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x92', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x93', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x94', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x94', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x98', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\x9b', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\x9c', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\x9c', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\x9d', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\x9e', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\x9f', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\xa6', '...').#\n",
    "            replace('\\\\xe2\\\\x80\\\\xb2', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb3', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb4', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb5', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb6', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb7', \"'\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xba', \"+\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xbb', \"-\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xbc', \"=\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xbd', \"(\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xbe', \")\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xa0', \"\").\n",
    "            replace('\\\\x0c', \"\").\n",
    "            replace('\\\\xef\\\\xac\\\\x81', \"fi\").\n",
    "            replace('\\\\ufb01', \"fi\").\n",
    "            # copyright symbol\n",
    "            replace('\\\\xc2\\\\xa9', \"\").\n",
    "            replace('\\\\xa9', \"\").\n",
    "            replace('\\\\xe2\\\\x89\\\\xa5', \"of\").\n",
    "            replace('\\\\xef\\\\xac\\\\x82', \"fl\").\n",
    "            # degree symbol\n",
    "            replace('\\\\xc2\\\\xb0', \" \").\n",
    "            replace('\\\\xb0', \" \").\n",
    "            replace('\\\\xe8', \"\").\n",
    "            replace('\\\\', \"\")\n",
    "\n",
    "    \n",
    "                 )\n",
    "    return TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_sentence(tokens):\n",
    "    \"\"\"\n",
    "    Takes as input a set of tokens from Stanford Core NLP output and returns \n",
    "    the set of peoplefound within the sentence. This relies on the fact that\n",
    "    named entities which are contiguous within a sentence should be part of \n",
    "    the same name. For example, in the following:\n",
    "    [\n",
    "        {'word': 'Brandon', 'ner': 'PERSON'},\n",
    "        {'word': 'Rose', 'ner': 'PERSON'},\n",
    "        {'word': 'eats', 'ner': 'O'},\n",
    "        {'word': 'bananas', 'ner': 'O'}\n",
    "    ]\n",
    "    we can safely assume that the contiguous PERSONs Brandon + Rose are part of the \n",
    "    same named entity, Brandon Rose.\n",
    "    \"\"\"\n",
    "    people = set()\n",
    "    token_count = 0\n",
    "    for i in range(len(tokens)):\n",
    "        if token_count < len(tokens):\n",
    "            person = ''\n",
    "            token = tokens[token_count]\n",
    "            if token['ner'] == 'PERSON':\n",
    "                person += token['word'].lower()\n",
    "                checking = True\n",
    "                while checking == True:\n",
    "                    if token_count + 1 < len(tokens):\n",
    "                        if tokens[token_count + 1]['ner'] == 'PERSON':\n",
    "                            token_count += 1\n",
    "                            person += ' {}'.format(tokens[token_count]['word'].lower())\n",
    "                        else:\n",
    "                            checking = False\n",
    "                            token_count += 1\n",
    "                    else:\n",
    "                        checking = False\n",
    "                        token_count += 1\n",
    "            else:\n",
    "                token_count += 1\n",
    "            if person != '':\n",
    "                people.add(person)\n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triples(tokens):\n",
    "    print(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "text = re.sub(r'\\\\n', \" \", df_topic_data.iloc[0]['raw_text']).strip()\n",
    "text = unicodetoascii(text)\n",
    "output = nlp.annotate(text, properties={\n",
    "  'annotators': 'parse',\n",
    "  'outputFormat': 'json'\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in document:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "token.ent_type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://spacy.io/docs/usage/processing-text\n",
    "document = nlp(text)\n",
    "\n",
    "#print('document: {0}'.format(document))\n",
    "\n",
    "# Load spacy's dependency tree into a networkx graph\n",
    "edges = []\n",
    "nodes = []\n",
    "for token in document:\n",
    "    # FYI https://spacy.io/docs/api/token\n",
    "        for child in token.children:\n",
    "            if not token.is_stop and not child.is_stop and token.pos_ not in [\"PUNCT\", \"SPACE\"] and child.pos_ not in [\"PUNCT\", \"SPACE\"]:\n",
    "                #edges.append(('{0}-{1}'.format(token.lower_,token.i),\n",
    "                #              '{0}-{1}'.format(child.lower_,child.i)))\n",
    "                nodes.append(('{0}-{1}'.format(token.lower_,token.i), {\"id\":token.i, \"name\":token.lower_, \"type\": token.pos_}))\n",
    "                nodes.append(('{0}-{1}'.format(child.lower_,child.i),{\"id\":child.i, \"name\":child.lower_, \"type\": child.pos_}))\n",
    "                #edges.append(('{0}-{1}'.format(token.lower_,token.i),\n",
    "                #              '{0}-{1}'.format(child.lower_,child.i)))\n",
    "                edges.append((token.i,child.i))\n",
    "\n",
    "graph = nx.Graph()\n",
    "graph.add_nodes_from(nodes)\n",
    "graph.add_edges_from(edges)\n",
    "\n",
    "# https://networkx.github.io/documentation/networkx-1.10/reference/algorithms.shortest_paths.html\n",
    "#print(nx.shortest_path_length(graph, source='robots-0', target='awesomeness-11'))\n",
    "#print(nx.shortest_path(graph, source='robots-0', target='awesomeness-11'))\n",
    "#print(nx.shortest_path(graph, source='robots-0', target='agency-15'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.readwrite import json_graph\n",
    "d = json_graph.node_link_data(graph) # node-link format to serialize\n",
    "\n",
    "json.dump(d, open('force/force.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe height=400px width=100% src='force/force.html'></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = corenlp.CoreNLPClient(annotators=\"tokenize ssplit depparse\".split(), timeout=35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.is_alive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.annotate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with corenlp.CoreNLPClient(annotators=\"parse\".split(), timeout=35000) as client:\n",
    "  ann = client.annotate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in output['sentences']:\n",
    "    triples = get_triples(sent['tokens'])\n",
    "    print(triples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
